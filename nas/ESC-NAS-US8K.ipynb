{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DR1TtXAtGas"
   },
   "source": [
    "## Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3288,
     "status": "ok",
     "timestamp": 1713849466409,
     "user": {
      "displayName": "Neural Dreamers",
      "userId": "16255580851036138152"
     },
     "user_tz": -330
    },
    "id": "cqVA8YbRuv-j"
   },
   "outputs": [],
   "source": [
    "# Opts\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "# Utils\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Train Generator\n",
    "import sys\n",
    "from tensorflow import keras\n",
    "\n",
    "# Trainer\n",
    "import glob\n",
    "import math\n",
    "import time\n",
    "\n",
    "# ESC-NAS\n",
    "import datetime\n",
    "import subprocess\n",
    "import gc\n",
    "import re\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import logging\n",
    "# tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "cd (Change directory) to the location of the folders\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/ESCNAS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1713849486965,
     "user": {
      "displayName": "Neural Dreamers",
      "userId": "16255580851036138152"
     },
     "user_tz": -330
    },
    "id": "HmPMsAv7qzy6"
   },
   "outputs": [],
   "source": [
    "def parse():\n",
    "    parser = argparse.ArgumentParser(description='ESC-NAS Sound Classification')\n",
    "\n",
    "    # General settings\n",
    "    parser.add_argument('--netType', default='ESC-NAS',  required=False)\n",
    "    parser.add_argument('--data', default='{}/datasets/'.format(os.getcwd()),  required=False)\n",
    "    parser.add_argument('--dataset', required=False, default='urbansound8k', choices=['fsc22', 'esc10', 'esc50', 'urbansound8k'])\n",
    "    parser.add_argument('--BC', default=True, action='store_true', help='BC learning')\n",
    "    parser.add_argument('--strongAugment', default=True,  action='store_true', help='Add scale and gain augmentation')\n",
    "\n",
    "    opt = parser.parse_args(args=[])\n",
    "\n",
    "    # Learning settings\n",
    "    opt.batchSize = 128\n",
    "    opt.weightDecay = 5e-4\n",
    "    opt.momentum = 0.9\n",
    "    opt.nEpochs = 150\n",
    "    opt.LR = 0.01\n",
    "    opt.schedule = [0.2, 0.4, 0.7]  # [0-10 = 10-3, 11-30 = 10-2, 31-60 = 10-3, 61-105=10-4, 106-150=10-5]\n",
    "    opt.warmup = 10\n",
    "\n",
    "    # Basic Net Settings\n",
    "    opt.nClasses = 10\n",
    "    opt.nFolds = 5\n",
    "    opt.splits = [i for i in range(1, opt.nFolds + 1)]\n",
    "    opt.sr = 20000\n",
    "    opt.inputLength = 30225\n",
    "    opt.mixupFactor = 2\n",
    "\n",
    "    # Test data\n",
    "    opt.nCrops = 10\n",
    "\n",
    "    opt.augmentation_data = {\"time_stretch\": 0.8, \"pitch_shift\": 1.5}\n",
    "\n",
    "    opt.class_labels = [\"air_conditioner\", \"car_horn\", \"children_playing\", \"dog_bark\", \"drilling\",\n",
    "                        \"engine_idling\", \"gun_shot\", \"jackhammer\", \"siren\", \"street_music\"]\n",
    "\n",
    "    return opt\n",
    "\n",
    "def display_info(opt):\n",
    "    print('+------------------------------+')\n",
    "    print('| {} Sound classification'.format(opt.netType))\n",
    "    print('+------------------------------+')\n",
    "    print('| dataset  : {}'.format(opt.dataset))\n",
    "    print('| nEpochs  : {}'.format(opt.nEpochs))\n",
    "    print('| LRInit   : {}'.format(opt.LR))\n",
    "    print('| schedule : {}'.format(opt.schedule))\n",
    "    print('| warmup   : {}'.format(opt.warmup))\n",
    "    print('| batchSize: {}'.format(opt.batchSize))\n",
    "    print('| Splits: {}'.format(opt.splits))\n",
    "    print('+------------------------------+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2HkhUcF1rMBD"
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1713849486965,
     "user": {
      "displayName": "Neural Dreamers",
      "userId": "16255580851036138152"
     },
     "user_tz": -330
    },
    "id": "knAGX_RFrKqT"
   },
   "outputs": [],
   "source": [
    "# Fixed seed for reproducability\n",
    "random.seed(42)\n",
    "# Default data augmentation\n",
    "def padding(pad):\n",
    "    def f(sound):\n",
    "        return np.pad(sound, pad, 'constant')\n",
    "\n",
    "    return f\n",
    "\n",
    "def random_crop(size):\n",
    "    def f(sound):\n",
    "        org_size = len(sound)\n",
    "        start = random.randint(0, org_size - size)\n",
    "        return sound[start: start + size]\n",
    "\n",
    "    return f\n",
    "\n",
    "def normalize(factor):\n",
    "    def f(sound):\n",
    "        return sound / factor\n",
    "\n",
    "    return f\n",
    "\n",
    "# For strong data augmentation\n",
    "def random_scale(max_scale, interpolate='Linear'):\n",
    "    def f(sound):\n",
    "        scale = np.power(max_scale, random.uniform(-1, 1))\n",
    "        output_size = int(len(sound) * scale)\n",
    "        ref = np.arange(output_size) / scale\n",
    "        if interpolate == 'Linear':\n",
    "            ref1 = ref.astype(np.int32)\n",
    "            ref2 = np.minimum(ref1 + 1, len(sound) - 1)\n",
    "            r = ref - ref1\n",
    "            scaled_sound = sound[ref1] * (1 - r) + sound[ref2] * r\n",
    "        elif interpolate == 'Nearest':\n",
    "            scaled_sound = sound[ref.astype(np.int32)]\n",
    "        else:\n",
    "            raise Exception('Invalid interpolation mode {}'.format(interpolate))\n",
    "\n",
    "        return scaled_sound\n",
    "\n",
    "    return f\n",
    "\n",
    "def random_gain(db):\n",
    "    def f(sound):\n",
    "        return sound * np.power(10, random.uniform(-db, db) / 20.0)\n",
    "\n",
    "    return f\n",
    "\n",
    "# For testing phase\n",
    "def multi_crop(input_length, n_crops):\n",
    "    def f(sound):\n",
    "        stride = (len(sound) - input_length) // (n_crops - 1)\n",
    "        sounds = [sound[stride * i: stride * i + input_length] for i in range(n_crops)]\n",
    "        return np.array(sounds)\n",
    "\n",
    "    return f\n",
    "\n",
    "# For BC learning\n",
    "def a_weight(fs, n_fft, min_db=-80.0):\n",
    "    freq = np.linspace(0, fs // 2, n_fft // 2 + 1)\n",
    "    freq_sq = np.power(freq, 2)\n",
    "    freq_sq[0] = 1.0\n",
    "    weight = 2.0 + 20.0 * (2 * np.log10(12194) + 2 * np.log10(freq_sq)\n",
    "                           - np.log10(freq_sq + 12194 ** 2)\n",
    "                           - np.log10(freq_sq + 20.6 ** 2)\n",
    "                           - 0.5 * np.log10(freq_sq + 107.7 ** 2)\n",
    "                           - 0.5 * np.log10(freq_sq + 737.9 ** 2))\n",
    "    weight = np.maximum(weight, min_db)\n",
    "\n",
    "    return weight\n",
    "\n",
    "def compute_gain(sound, fs, min_db=-80.0, mode='A_weighting'):\n",
    "    if fs == 16000 or fs == 20000:\n",
    "        n_fft = 2048\n",
    "    elif fs == 44100:\n",
    "        n_fft = 4096\n",
    "    else:\n",
    "        raise Exception('Invalid fs {}'.format(fs))\n",
    "    stride = n_fft // 2\n",
    "\n",
    "    gain = []\n",
    "\n",
    "    for i in range(0, len(sound) - n_fft + 1, stride):\n",
    "        if mode == 'RMSE':\n",
    "            g = np.mean(sound[i: i + n_fft] ** 2)\n",
    "        elif mode == 'A_weighting':\n",
    "            spec = np.fft.rfft(np.hanning(n_fft + 1)[:-1] * sound[i: i + n_fft])\n",
    "            power_spec = np.abs(spec) ** 2\n",
    "            a_weighted_spec = power_spec * np.power(10, a_weight(fs, n_fft) / 10)\n",
    "            g = np.sum(a_weighted_spec)\n",
    "        else:\n",
    "            raise Exception('Invalid mode {}'.format(mode))\n",
    "        gain.append(g)\n",
    "\n",
    "    gain = np.array(gain)\n",
    "    gain = np.maximum(gain, np.power(10, min_db / 10))\n",
    "    gain_db = 10 * np.log10(gain)\n",
    "\n",
    "    return gain_db\n",
    "\n",
    "def mix(sound1, sound2, r, fs):\n",
    "    gain1 = np.max(compute_gain(sound1, fs))  # Decibel\n",
    "    gain2 = np.max(compute_gain(sound2, fs))\n",
    "    t = 1.0 / (1 + np.power(10, (gain1 - gain2) / 20.) * (1 - r) / r)\n",
    "    sound = ((sound1 * t + sound2 * (1 - t)) / np.sqrt(t ** 2 + (1 - t) ** 2))\n",
    "\n",
    "    return sound\n",
    "\n",
    "# Convert time representation\n",
    "def to_hms(time):\n",
    "    h = int(time // 3600)\n",
    "    m = int((time - h * 3600) // 60)\n",
    "    s = int(time - h * 3600 - m * 60)\n",
    "    if h > 0:\n",
    "        line = '{}h{:02d}m'.format(h, m)\n",
    "    else:\n",
    "        line = '{}m{:02d}s'.format(m, s)\n",
    "\n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQ753acQtKIj"
   },
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KeUv9sGtC8r"
   },
   "source": [
    "### Train Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 675,
     "status": "ok",
     "timestamp": 1713849487638,
     "user": {
      "displayName": "Neural Dreamers",
      "userId": "16255580851036138152"
     },
     "user_tz": -330
    },
    "id": "_9A2iLGIre3b"
   },
   "outputs": [],
   "source": [
    "class Generator(keras.utils.Sequence):\n",
    "    # Generates data for Keras\n",
    "    def __init__(self, samples, labels, options):\n",
    "        self.data = [(samples[i], labels[i]) for i in range (0, len(samples))]\n",
    "        self.opt = options\n",
    "        self.batch_size = options.batchSize\n",
    "        self.preprocess_funcs = self.preprocess_setup()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Denotes the number of batches per epoch\n",
    "        return int(np.floor(len(self.data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, batchIndex):\n",
    "        # Generate one batch of data\n",
    "        batchX, batchY = self.generate_batch(batchIndex)\n",
    "        batchX = np.expand_dims(batchX, axis=1)\n",
    "        batchX = np.expand_dims(batchX, axis=3)\n",
    "        return batchX, batchY\n",
    "\n",
    "    def generate_batch(self, batchIndex):\n",
    "        # Generates data containing batch_size samples\n",
    "        sounds = []\n",
    "        labels = []\n",
    "        selected = []\n",
    "        indexes = None\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            # Training phase of BC learning\n",
    "            # Select two training examples\n",
    "            while True:\n",
    "                ind1 = random.randint(0, len(self.data) - 1)\n",
    "                ind2 = random.randint(0, len(self.data) - 1)\n",
    "\n",
    "                sound1, label1 = self.data[ind1]\n",
    "                sound2, label2 = self.data[ind2]\n",
    "\n",
    "                if len({label1, label2}) == 2 and \"{}-{}\".format(ind1, ind2) not in selected:\n",
    "                    selected.append(\"{}-{}\".format(ind1, ind2))\n",
    "                    break\n",
    "            sound1 = self.preprocess(sound1)\n",
    "            sound2 = self.preprocess(sound2)\n",
    "\n",
    "            # Mix two examples\n",
    "            r = np.array(random.random())\n",
    "            sound = mix(sound1, sound2, r, self.opt.sr).astype(np.float32)\n",
    "            eye = np.eye(self.opt.nClasses)\n",
    "            label = (eye[label1 - 1] * r + eye[label2 - 1] * (1 - r)).astype(np.float32)\n",
    "\n",
    "            # For stronger augmentation\n",
    "            sound = random_gain(6)(sound).astype(np.float32)\n",
    "\n",
    "            sounds.append(sound)\n",
    "            labels.append(label)\n",
    "\n",
    "        sounds = np.asarray(sounds)\n",
    "        labels = np.asarray(labels)\n",
    "\n",
    "        return sounds, labels\n",
    "\n",
    "    def preprocess_setup(self):\n",
    "        funcs = []\n",
    "        if self.opt.strongAugment:\n",
    "            funcs += [random_scale(1.25)]\n",
    "\n",
    "        funcs += [padding(self.opt.inputLength // 2),\n",
    "                  random_crop(self.opt.inputLength),\n",
    "                  normalize(32768.0)]\n",
    "        return funcs\n",
    "\n",
    "    def preprocess(self, sound):\n",
    "        for f in self.preprocess_funcs:\n",
    "            sound = f(sound)\n",
    "\n",
    "        return sound\n",
    "\n",
    "def setup(opt, split):\n",
    "    dataset = np.load(os.path.join(opt.data, opt.dataset, 'wav{}.npz'.format(opt.sr // 1000)), allow_pickle=True)\n",
    "    train_sounds = []\n",
    "    train_labels = []\n",
    "    \n",
    "    for i in range(1, opt.nFolds + 1):\n",
    "        sounds = dataset['fold{}'.format(i)].item()['sounds']\n",
    "        labels = dataset['fold{}'.format(i)].item()['labels']\n",
    "        if i != split:\n",
    "            train_sounds.extend(sounds)\n",
    "            train_labels.extend(labels)\n",
    "\n",
    "    trainGen = Generator(train_sounds, train_labels, opt)\n",
    "\n",
    "    return trainGen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QM9c5xbtOdU"
   },
   "source": [
    "## TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cS4FT41rZTz"
   },
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1713849487638,
     "user": {
      "displayName": "Neural Dreamers",
      "userId": "16255580851036138152"
     },
     "user_tz": -330
    },
    "id": "nZC_RwtYrYMb"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, opt=None):\n",
    "        self.opt = opt\n",
    "        self.trainGen = setup(self.opt, self.opt.split)\n",
    "\n",
    "    def GetLR(self, epoch):\n",
    "        divide_epoch = np.array([self.opt.nEpochs * i for i in self.opt.schedule])\n",
    "        decay = sum(epoch > divide_epoch)\n",
    "        if epoch <= self.opt.warmup:\n",
    "            decay = 1\n",
    "        return self.opt.LR * np.power(0.1, decay)\n",
    "\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "        self.testX = None\n",
    "        self.testY = None\n",
    "        self.curEpoch = 0\n",
    "        self.curLr = opt.LR\n",
    "        self.cur_epoch_start_time = time.time()\n",
    "        self.bestAcc = 0.0\n",
    "        self.bestAccEpoch = 0\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.curEpoch = epoch+1\n",
    "        self.curLr = Trainer(self.opt).GetLR(epoch+1)\n",
    "        self.cur_epoch_start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        train_time = time.time() - self.cur_epoch_start_time\n",
    "        self.load_test_data()\n",
    "        val_acc, val_loss = self.validate(self.model)\n",
    "        logs['val_acc'] = val_acc\n",
    "        logs['val_loss'] = val_loss\n",
    "        if val_acc > self.bestAcc:\n",
    "            self.bestAcc = val_acc\n",
    "            self.bestAccEpoch = epoch + 1\n",
    "        epoch_time = time.time() - self.cur_epoch_start_time\n",
    "        val_time = epoch_time - train_time\n",
    "        line = 'SP-{}, Epoch: {}/{} | Time: {} (Train {}  Val {}) | Train: LR {}  Loss {:.2f}  Acc {:.2f}% | Val: Loss {:.2f}  Acc(top1) {:.2f}% | HA {:.2f}@{}\\n'.format(\n",
    "            self.opt.split, epoch+1, self.opt.nEpochs, to_hms(epoch_time), to_hms(train_time), to_hms(val_time),\n",
    "            self.curLr, logs['loss'], logs['accuracy']*100 if 'accuracy' in logs else logs['acc']*100, val_loss, val_acc, self.bestAcc, self.bestAccEpoch)\n",
    "        sys.stdout.write(line)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    def load_test_data(self):\n",
    "        if self.testX is None:\n",
    "            data = np.load(os.path.join(self.opt.data, self.opt.dataset,\n",
    "                                        'test_data_{}khz/fold{}_test800.npz'.format(self.opt.sr // 1000,\n",
    "                                                                                     self.opt.split)),\n",
    "                           allow_pickle=True)\n",
    "\n",
    "            self.testX = data['x']\n",
    "            self.testY = data['y']\n",
    "\n",
    "    def validate(self, model):\n",
    "        y_pred = None\n",
    "        y_target = None\n",
    "        batch_size = (self.opt.batchSize//self.opt.nCrops)*self.opt.nCrops\n",
    "        \n",
    "        for batchIndex in range(math.ceil(len(self.testX) / batch_size)):\n",
    "            x = self.testX[batchIndex*batch_size : (batchIndex+1)*batch_size]\n",
    "            y = self.testY[batchIndex*batch_size : (batchIndex+1)*batch_size]\n",
    "            scores = model.predict(x, batch_size=len(y), verbose=0)\n",
    "            y_pred = scores if y_pred is None else np.concatenate((y_pred, scores))\n",
    "            y_target = y if y_target is None else np.concatenate((y_target, y))\n",
    "\n",
    "        acc, loss = self.compute_accuracy(y_pred, y_target)\n",
    "        return acc, loss\n",
    "\n",
    "    # Calculating average prediction (10 crops) and final accuracy\n",
    "    def compute_accuracy(self, y_pred, y_target):\n",
    "        # Reshape y_pred to shape it like each sample comtains 10 samples.\n",
    "        if self.opt.nCrops > 1:\n",
    "            y_pred = (y_pred.reshape(y_pred.shape[0]//self.opt.nCrops, self.opt.nCrops, y_pred.shape[1])).mean(axis=1)\n",
    "            y_target = (y_target.reshape(y_target.shape[0]//self.opt.nCrops, self.opt.nCrops, y_target.shape[1])).mean(axis=1)\n",
    "\n",
    "        loss = keras.losses.KLD(y_target, y_pred).numpy().mean()\n",
    "\n",
    "        #Get the indices that has highest average value for each sample\n",
    "        y_pred = y_pred.argmax(axis=1) + 1\n",
    "        y_target = y_target.argmax(axis=1) + 1\n",
    "        accuracy = (y_pred==y_target).mean()*100\n",
    "\n",
    "        return accuracy, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qH0qtopG0F-r"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1713849487639,
     "user": {
      "displayName": "Neural Dreamers",
      "userId": "16255580851036138152"
     },
     "user_tz": -330
    },
    "id": "iaFpddQbtiys"
   },
   "outputs": [],
   "source": [
    "class ESCNAS_mixup :\n",
    "    architecture_name = 'resulting_architecture'\n",
    "    def __init__(self, max_RAM, max_Flash, val_split, input_shape, save_path='./', opt=None, n_class=10) :\n",
    "        self.max_Flash = max_Flash\n",
    "        self.max_RAM = max_RAM\n",
    "        self.num_classes = n_class\n",
    "        self.val_split = val_split\n",
    "        self.input_shape = input_shape\n",
    "        self.save_path = save_path\n",
    "\n",
    "        self.path_to_trained_models = f\"{self.save_path}/trained_models\"\n",
    "        os.makedirs(self.path_to_trained_models)\n",
    "\n",
    "        display_info(opt)\n",
    "        self.trainer = Trainer(opt)\n",
    "\n",
    "    # k base number of kernels of the convolutional layers\n",
    "    # c number of cells added upon the first convolutional layer\n",
    "    def Model(self, k, c) :\n",
    "        kernel_size = (3,3)\n",
    "        pool_size = (2,2)\n",
    "        pool_strides = (2,2)\n",
    "        number_of_cells_limited = False\n",
    "\n",
    "        inputs = tf.keras.Input(shape=self.input_shape)\n",
    "\n",
    "        # convolutional base\n",
    "        n = k\n",
    "        multiplier = 2\n",
    "\n",
    "        # ACFE block\n",
    "        acfe = tf.keras.layers.Conv2D(filters=8, kernel_size=(1,9), strides=(1,pool_strides[0]), padding='valid',\n",
    "                                      kernel_initializer=tf.keras.initializers.he_normal(), use_bias=False)(inputs)\n",
    "        acfe = tf.keras.layers.BatchNormalization()(acfe)\n",
    "        acfe = tf.keras.layers.ReLU()(acfe)\n",
    "\n",
    "        acfe = tf.keras.layers.Conv2D(filters=64, kernel_size=(1,5), strides=(1,pool_strides[1]), padding='valid',\n",
    "                                      kernel_initializer=tf.keras.initializers.he_normal(), use_bias=False)(acfe)\n",
    "        acfe = tf.keras.layers.BatchNormalization()(acfe)\n",
    "        acfe = tf.keras.layers.ReLU()(acfe)\n",
    "\n",
    "        acfe = tf.keras.layers.MaxPooling2D(pool_size=(1,n))(acfe)\n",
    "\n",
    "        x = tf.keras.layers.Permute((3, 2, 1))(acfe)\n",
    "\n",
    "        # TDFE block\n",
    "        # first convolutional layer\n",
    "        x = tf.keras.layers.Conv2D(filters=32, kernel_size=kernel_size, strides=(1,1), padding='same',\n",
    "                                      kernel_initializer=tf.keras.initializers.he_normal(), use_bias=False)(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "        x = tf.keras.layers.MaxPooling2D(pool_size=pool_size)(x)\n",
    "\n",
    "        # adding cells into TDFE\n",
    "        for i in range(1, c + 1) :\n",
    "            if x.shape[1] <= 1 or x.shape[2] <= 1 :\n",
    "                number_of_cells_limited = True\n",
    "                break;\n",
    "            n = np.ceil(n * multiplier)\n",
    "            multiplier = multiplier - 2**-i\n",
    "            \n",
    "            x = tf.keras.layers.Conv2D(filters=n, kernel_size=kernel_size, strides=(1,1), padding='same',\n",
    "                                      kernel_initializer=tf.keras.initializers.he_normal(), use_bias=False)(x)\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "            x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "            x = tf.keras.layers.MaxPooling2D(pool_size=pool_size)(x)\n",
    "        \n",
    "        # classifier\n",
    "        x = tf.keras.layers.Dropout(rate=0.2)(x)\n",
    "\n",
    "        x = tf.keras.layers.Conv2D(filters=self.num_classes, kernel_size=(1,1), strides=(1,1), padding='valid',\n",
    "                                      kernel_initializer=tf.keras.initializers.he_normal(), use_bias=False)(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "        x = tf.keras.layers.Dense(self.num_classes, kernel_initializer=tf.keras.initializers.he_normal())(x)\n",
    "        outputs = tf.keras.layers.Softmax()(x)\n",
    "\n",
    "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        return model, number_of_cells_limited\n",
    "\n",
    "    def plot_history(self, hist, model_name, model_save_dir, f=1):\n",
    "        run_epochs = len(hist.history['accuracy'])\n",
    "        tr_acc = hist.history['accuracy']\n",
    "        tr_loss = hist.history['loss']\n",
    "        val_acc = hist.history['val_acc']\n",
    "        val_loss = hist.history['val_loss']\n",
    "\n",
    "        save_epochs = [i for i in range(0, run_epochs, f)]\n",
    "        save_tr_acc = [tr_acc[i]*100 for i in range(0, run_epochs, f)]\n",
    "        save_tr_loss = [tr_loss[i] for i in range(0, run_epochs, f)]\n",
    "        save_val_acc = [val_acc[i] for i in range(0, run_epochs, f)]\n",
    "        save_val_loss = [val_loss[i] for i in range(0, run_epochs, f)]\n",
    "\n",
    "        # Create a figure and axis\n",
    "        fig, ax1 = plt.subplots()\n",
    "\n",
    "        fig.set_figheight(12)\n",
    "        fig.set_figwidth(24)\n",
    "\n",
    "        # Plot accuracy lines\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Accuracy', color='black')\n",
    "        ax1.plot(save_epochs, save_tr_acc, color='#800000', marker='o', label='Training Accuracy')\n",
    "        ax1.plot(save_epochs, save_val_acc, color='#000075', marker='x', label='Validation Accuracy')\n",
    "        # ax1.set_xticklabels(save_epochs, rotation=90)\n",
    "\n",
    "        # Create a second y-axis for loss lines\n",
    "        ax2 = ax1.twinx()  # Share the same x-axis\n",
    "        ax2.set_ylabel('Loss', color='black')\n",
    "        ax2.plot(save_epochs, save_tr_loss, color='#3cb44b', marker='s', label='Training Loss')\n",
    "        ax2.plot(save_epochs, save_val_loss, color='#f58231', marker='^', label='Validation Loss')\n",
    "        ax2.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "        # Add a legend\n",
    "        lines, labels = ax1.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax2.legend(lines + lines2, labels + labels2, loc='best')\n",
    "\n",
    "        # Set a title\n",
    "        plt.title('Accuracy and Loss Over Epochs')\n",
    "\n",
    "        accuracy_matrices_path = model_save_dir\n",
    "\n",
    "        curr_datetime = datetime.datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "        filename = f'{model_name.lower()}-training_metrics_plot-{format(curr_datetime)}.png'\n",
    "\n",
    "        if not os.path.exists(accuracy_matrices_path):\n",
    "            os.makedirs(accuracy_matrices_path)\n",
    "\n",
    "        # Save the plot to the specified folder\n",
    "        destination = os.path.join(accuracy_matrices_path, filename)\n",
    "        plt.savefig(destination, bbox_inches='tight')\n",
    "\n",
    "    def evaluate_flash_and_peak_RAM_occupancy(self, custom_evaluator) :\n",
    "        # quantize model to evaluate its peak RAM occupancy and its Flash occupancy\n",
    "        self.quantize_model_uint8(custom_evaluator)\n",
    "\n",
    "        # evaluate its peak RAM occupancy and its Flash occupancy using STMicroelectronics' X-CUBE-AI\n",
    "        proc = subprocess.Popen([\"./stm32tflm\", f\"{self.path_to_trained_models}/{self.model_name}_quantized.tflite\"], stdout=subprocess.PIPE)\n",
    "        try:\n",
    "            outs, errs = proc.communicate(timeout=15)\n",
    "            Flash, RAM = re.findall(r'\\d+', str(outs))\n",
    "        except subprocess.TimeoutExpired:\n",
    "            proc.kill()\n",
    "            outs, errs = proc.communicate()\n",
    "            print(\"stm32tflm error\")\n",
    "            exit()\n",
    "\n",
    "        return int(Flash), int(RAM)\n",
    "\n",
    "    def quantize_model_uint8(self, custom_evaluator):\n",
    "        def representative_dataset():\n",
    "            rebatched_ds = tf.data.Dataset.from_tensor_slices((custom_evaluator.testX)).batch(1).take(100).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "            for data in rebatched_ds:\n",
    "                yield [data]\n",
    "\n",
    "        model = tf.keras.models.load_model(f\"{self.path_to_trained_models}/{self.model_name}.h5\")\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.representative_dataset = representative_dataset\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "        converter.inference_input_type = tf.uint8\n",
    "        converter.inference_output_type = tf.uint8\n",
    "        tflite_quant_model = converter.convert()\n",
    "\n",
    "        with open(f\"{self.path_to_trained_models}/{self.model_name}_quantized.tflite\", 'wb') as f:\n",
    "            f.write(tflite_quant_model)\n",
    "\n",
    "        # os.remove(f\"{self.path_to_trained_models}/{self.model_name}.h5\")\n",
    "\n",
    "    def evaluate_model_process(self, k, c) :\n",
    "        if k > 0 :\n",
    "            self.model_name = f\"k_{k}_c_{c}\"\n",
    "            print(f\"\\n{self.model_name}\\n\")\n",
    "\n",
    "            checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "                f\"{self.path_to_trained_models}/{self.model_name}.h5\",\n",
    "                monitor='val_acc', save_best_only=True, save_weights_only=False, mode='auto')\n",
    "\n",
    "            model, number_of_cells_limited = self.Model(k, c)\n",
    "            model.summary()\n",
    "\n",
    "            loss = 'kullback_leibler_divergence'\n",
    "            optimizer = keras.optimizers.SGD(learning_rate=self.trainer.opt.LR,\n",
    "                                             weight_decay=self.trainer.opt.weightDecay,\n",
    "                                             momentum=self.trainer.opt.momentum, nesterov=True)\n",
    "            model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "            print(f'number_of_cells_limited: {number_of_cells_limited}')\n",
    "            # exit()\n",
    "\n",
    "            # learning schedule callback\n",
    "            lrate = keras.callbacks.LearningRateScheduler(self.trainer.GetLR)\n",
    "            custom_evaluator = CustomCallback(self.trainer.opt)\n",
    "\n",
    "            callbacks_list = [lrate, custom_evaluator, checkpoint]\n",
    "\n",
    "            # One epoch of training must be done before quantization, which is needed to evaluate RAM and Flash occupancy\n",
    "            model.fit(self.trainer.trainGen, epochs=1,\n",
    "                      steps_per_epoch=len(self.trainer.trainGen.data)//self.trainer.trainGen.batch_size,\n",
    "                      callbacks=callbacks_list, verbose=0)\n",
    "\n",
    "            model.save(f\"{self.path_to_trained_models}/{self.model_name}.h5\")\n",
    "\n",
    "            Flash, RAM = self.evaluate_flash_and_peak_RAM_occupancy(custom_evaluator)\n",
    "            print(f\"\\nRAM: {RAM},\\t Flash: {Flash}\\n\")\n",
    "\n",
    "            if Flash <= self.max_Flash and RAM <= self.max_RAM and not number_of_cells_limited :\n",
    "                hist = model.fit(self.trainer.trainGen, epochs=self.trainer.opt.nEpochs,\n",
    "                      steps_per_epoch=len(self.trainer.trainGen.data)//self.trainer.trainGen.batch_size,\n",
    "                      callbacks=callbacks_list, verbose=0)\n",
    "\n",
    "                self.plot_history(hist, self.model_name, self.path_to_trained_models, 5)\n",
    "                self.quantize_model_uint8(custom_evaluator)\n",
    "\n",
    "            return {'k': k,\n",
    "                    'c': c if not number_of_cells_limited else \"Not feasible\",\n",
    "                    'RAM': RAM if RAM <= self.max_RAM else \"Outside the upper bound\",\n",
    "                    'Flash': Flash if Flash <= self.max_Flash else \"Outside the upper bound\",\n",
    "                    'max_val_acc':\n",
    "                    np.around(np.amax(hist.history['val_acc']), decimals=3)\n",
    "                    if 'hist' in locals() else -3}\n",
    "        else :\n",
    "            return{'k': 'unfeasible', 'c': c, 'max_val_acc': -3}\n",
    "\n",
    "    def explore_num_cells(self, k) :\n",
    "        previous_architecture = {'k': -1, 'c': -1, 'max_val_acc': -2}\n",
    "        current_architecture = {'k': -1, 'c': -1, 'max_val_acc': -1}\n",
    "        c = 1\n",
    "        k = int(k)\n",
    "\n",
    "        while(current_architecture['max_val_acc'] > previous_architecture['max_val_acc']) :\n",
    "            previous_architecture = current_architecture\n",
    "            c = c + 1\n",
    "            self.model_counter = self.model_counter + 1\n",
    "            current_architecture = self.evaluate_model_process(k, c)\n",
    "            print(f\"\\n\\n\\n{current_architecture}\\n\\n\\n\")\n",
    "        return previous_architecture\n",
    "\n",
    "    def search(self) :\n",
    "        self.model_counter = 0\n",
    "        epsilon = 0.005\n",
    "        k0 = 8\n",
    "\n",
    "        start = datetime.datetime.now()\n",
    "\n",
    "        k = k0\n",
    "        previous_architecture = self.explore_num_cells(k)\n",
    "        k = 2 * k\n",
    "        current_architecture = self.explore_num_cells(k)\n",
    "\n",
    "        if (current_architecture['max_val_acc'] > previous_architecture['max_val_acc']) :\n",
    "            previous_architecture = current_architecture\n",
    "            k = 2 * k\n",
    "            current_architecture = self.explore_num_cells(k)\n",
    "            while(current_architecture['max_val_acc'] > previous_architecture['max_val_acc'] + epsilon) :\n",
    "                previous_architecture = current_architecture\n",
    "                k = 2 * k\n",
    "                current_architecture = self.explore_num_cells(k)\n",
    "        else :\n",
    "            k = k0 / 2\n",
    "            current_architecture = self.explore_num_cells(k)\n",
    "            while(current_architecture['max_val_acc'] >= previous_architecture['max_val_acc']) :\n",
    "                previous_architecture = current_architecture\n",
    "                k = k / 2\n",
    "                current_architecture = self.explore_num_cells(k)\n",
    "\n",
    "        resulting_architecture = previous_architecture\n",
    "\n",
    "        end = datetime.datetime.now()\n",
    "\n",
    "        if (resulting_architecture['max_val_acc'] > 0) :\n",
    "            resulting_architecture_name = f\"k_{resulting_architecture['k']}_c_{resulting_architecture['c']}_quantized.tflite\"\n",
    "            self.path_to_resulting_architecture = f\"{self.save_path}/resulting_architecture_{resulting_architecture_name}\"\n",
    "            print(f\"\\nResulting architecture: {resulting_architecture}\\n\")\n",
    "        else :\n",
    "            print(f\"\\nNo feasible architecture found\\n\")\n",
    "        print(f\"Elapsed time (search): {end-start}\\n\")\n",
    "\n",
    "        return self.path_to_resulting_architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMDEr2VTugAM"
   },
   "source": [
    "Enable stm32tflm script execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 450,
     "status": "ok",
     "timestamp": 1713849488085,
     "user": {
      "displayName": "Neural Dreamers",
      "userId": "16255580851036138152"
     },
     "user_tz": -330
    },
    "id": "HA7Lyn7LuQkW"
   },
   "outputs": [],
   "source": [
    "!chmod +x stm32tflm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCal8zaXuYaK"
   },
   "source": [
    "Run ESC-NAS-Mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59049,
     "status": "ok",
     "timestamp": 1713849547131,
     "user": {
      "displayName": "Neural Dreamers",
      "userId": "16255580851036138152"
     },
     "user_tz": -330
    },
    "id": "b4ysxk6yyqD0",
    "outputId": "a2da2401-80c7-4d7f-9027-3a368c1be279"
   },
   "outputs": [],
   "source": [
    "opt = parse()\n",
    "opt.sr = 20000\n",
    "opt.inputLength = 30225\n",
    "opt.split = 1\n",
    "input_shape = (1, opt.inputLength, 1)\n",
    "\n",
    "# 5 MB RAM, 150 kB Flash\n",
    "peak_RAM_upper_bound = 1024 * 1024 * 5\n",
    "Flash_upper_bound = 1024 * 150\n",
    "\n",
    "val_split = 0.2\n",
    "\n",
    "# save results\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "save_path = './results_US8K_' + timestamp\n",
    "\n",
    "# show the GPU used\n",
    "!nvidia-smi\n",
    "\n",
    "ESCNAS_mixup = ESCNAS_mixup(peak_RAM_upper_bound, Flash_upper_bound,\n",
    "                            val_split, input_shape, save_path=save_path, opt=opt, n_class=opt.nClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ljO3uVoIB_AP",
    "outputId": "ee096542-6035-4b94-eb3b-5ebe5d532164"
   },
   "outputs": [],
   "source": [
    "# search\n",
    "path_to_tflite_model = ESCNAS_mixup.search()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
